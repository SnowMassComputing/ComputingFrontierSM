\textcolor{red}{More speculative section needs feedback from co-authors. \ldots}

Beyond the five-year timescale, concrete projections for the physics
capabilities and computing needs of numerical lattice calculations become less
reliable and more speculative.  Lattice field theory is a theoretical area of
research, and the development of new lattice formulations and analysis methods
as well as better computing algorithms drive rapid, but
difficult-to-anticipate, evolution of the field.  Here we attempt to
extrapolate to the extended time period covered by the Snowmass study based on
existing lattice methods and increased computing resources.  For concreteness,
we focus on weak matrix-element calculations, for which current lattice
results are the most precise and there is the most quantitative experience.

We begin with the conservative assumptions that exascale performance
($10^{18}$ flops/second) will be achieved by 2022, and that a further factor
of 100 will be available by 2032.  Present large-scale lattice calculations at
physical quark masses are performed in volumes of linear size $L \approx 6$ fm
and with inverse lattice spacing $1/a$ as small as $\sim 2.5$ GeV.  Thus,
these $10^2$ and $10^4$ advances in computer capability will allow an increase
in physical volume to 15 and 36 fm and in inverse lattice spacing to 5 and 10
GeV, respectively.  Statistical errors can be reduced by a factor of ten, or
even one hundred, as needed.  These three directions of substantial increase
in capability translate directly into physics opportunities.  The large
increase in possible Monte Carlo statistics will enable a reduction in the
errors on many nucleon matrix elements to the percent level, and on quark
flavor-changing matrix elements to the sub-percent level.  Such increased
statistics will also directly support perhaps few-percent precision for
results that depend on quark-disconnected diagrams such as $\epsilon'$ and the
$K_L-K_S$ mass difference.  For most QCD calculations, the non-zero pion mass
implies that finite volume effects decrease exponentially in the linear size
of the system.  However, this situation changes dramatically when
electromagnetic effects are included.  Here the massless photon leads to
substantial finite volume errors which decrease only as a power of $L$ as the
linear system size $L$ becomes large.  The ability to work on systems of
linear size 20 or 30 fm will play an important role in both better
understanding electromagnetic effects using lattice methods, and achieving the
10\% errors in the computation of such effects that are needed to attain 0.1\%
overall errors in quantities such as the light-quark-mass ratio $m_u/m_d$ and
the leptonic decay-constant ratio $f_K/f_\pi$.  Finally the ability to work
with an inverse lattice spacing as large as 10 GeV will allow substantial
improvements in the treatment of charm and bottom quarks, and enable
determinations of many quantities involving $B$ and $D$ mesons with errors
well below 1\%.

Clearly an enhanced computational capability of four orders of magnitude,
coupled with possibly equally large theoretical and algorithmic advances, will
have a dramatic effect on the phenomena that can be analyzed and precision
that can be achieved using lattice methods.  The possibility of making
Standard-Model predictions with errors that are an order-of-magnitude smaller
than present experimental errors will create an exciting challenge to identify
quantities where substantially increased experimental precision is possible.
With the ability to make highly accurate Standard-Model predictions for a
growing range of quantities, experiments can be designed to target those
quantities that are potentially most sensitive to physics
beyond-the-Standard-Model, rather than being limited to those quantities which
are least obscured by the effects of QCD.
