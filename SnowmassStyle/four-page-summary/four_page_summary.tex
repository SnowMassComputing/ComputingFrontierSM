Computing has become a major component of all particle physics
experiments and in many areas of theoretical particle physics.
The computing frontier initially established seven subgroups
covering user needs and four covering infrastructure.  
The experimental user needs groups
covered each of the experimental frontiers: cosmic, energy and intensity.
Theoretical subgroups covering accelerator science; astrophysics and
cosmology; lattice field theory; and perturbative QCD were formed.  However,
astrophysics and cosmology decided to join forces with the cosmic frontier and produced
a joint report covering both experiment/observation and theory.
The infrastructure groups examined trends in computing
to predict how technology will evolve and how it will effect future
costs and capabilities.
The four infrastructure groups focused on distributed computing 
and facility infrastructures; networking;
software development, personnel,  and training; and
data management and storage.
Another goal of the infrastructure groups was to identify
critical needs that might require the DOE or NSF to fund
research in computer technology or computer science needed by
particle physicists.

During the period between the Community Planning Meeting at
Fermilab and the Community Summer Study at the University of
Minnesota, the user groups were actively engaged with other
Snowmass groups in order to learn of their plans and estimate their
computing needs.  The infrastructure groups were engaged with
vendors, computer users and providers and technical experts to
try to predict trends in computing, networking, storage, and software
development, including considerations of
costs, capacities and speeds.  At the University of Minnesota our
first day of parallel sessions was devoted to the users presenting
their future needs.  The next day the infrastructure groups
reported on their findings and reflected on the user needs.

It is clear that progress in both experiment and theory will require
significantly more computing, software development, storage and networking, 
with different projects stretching future capabilities in different ways.
To move ahead in the most efficient manner will require careful and
continuing review of the topics we studied, {\it i.e.}, user needs and
capabilities of current and future technology.  
For many years, the particle physics community has been a great source 
of computing innovation and expertise.  It is essential to leverage those
assets through wider sharing of knowledge throughout the experimental
and theoretical communities.  In fact, we should be sharing our
expertise with the entire scientific community.

The experimental program relies for the most part on distributed
high-throughput computing (HTP).  The distinction here is that 
individual events are analyzed and that the analyses are
independent of each other.  By distributed, we mean that a groups
of events can be assigned to hardware in different locations and
results can be combined together when all groups are done.  This
is the computing model for ATLAS and CMS, with Tier 0, Tier 1 and
Tier 2 centers around the globe.

The theoretical computing needs are more commonly considered high-performance
computing (HPC) in which thousands to hundreds of thousands of tightly
coupled CPUs are working simultaneously on a single problem.

One issue for the HTP computing side is to what degree it makes sense to
use national supercomputer centers which have traditionally been designed
for HPC usage.  A second pressing issue is faced by both communities.
It has been clear for the past several years that chip speeds are not
increasing as they were for at least two decades.  Thus, we cannot
rely on new hardware to run our serial codes faster.  New chips
have multiple cores, and we must rely on parallel codes to increase
performance.  In addition to multi-core chips, there are now
accelerators such a graphical processing units (GPUs) and many-core
chips such as the Intel Xeon Phi has about 60 cores.
Given these changes in chip technology and evolution of
high performance systems,
how will we write the parallel codes that will be
needed in the future, and how will we train the personnel to
develop, support and maintain them?
Different subgroups are at different stages in their efforts to
port to these new technologies.  Lattice QCD, for example,
started its GPU porting efforts in 2008 and has had code in 
production for some time, particularly for quark inversions;
however, there are other parts of the code that are still only
running on CPUs.  Several key developers moved to
NVIDIA or Google.  Fortunately, the people who work at NVIDIA
at still actively involved with the QCD effort,  but only a limited number of
people in the field can program GPUs.
Accelerator science is also actively porting codes to GPUs, in 
particular some of the solvers and particle-in-cell infrastructure
have been ported.  Speedups of 50 compared to a single core and
4 compared to an OpenMP code 12-core have been seen.
Perturbative QCD has also started using GPUs.
Further details can be found in the subgroup reports.

There are training materials from some of the national
supercomputing centers and summer schools are organized
by, among others, the Virtual School of Computational
Science and Engineering (www.vscse.org), but we need to examine whether these
provide the right training for our field and whether the
delivery mechanisms are timely.  Using on-line media, workbooks
and wikis were suggested to enhance training.  Another area of
common concern is the career path of those who become experts in
software development.  It is useful to help young scientists learn
computing and software skills that are marketable for non-academic
jobs, but it is also important that there be career paths within particle
physics, including tenure-track jobs, for those working at the
forefront of computation.

At the LHC, ATLAS and CMS have to limit their data rates by
triggering, so computing limitations already cause a reduction in
the data that is analyzed.
Upgrades to the LHC in energy or luminosity will place more 
demands on the distributed computing systems for ATLAS and CMS.  
There is a potential for a ten-fold increase in the number of events
and a ten-fold increase in event complexity.  Effort has begun to 
increase parallelism in reconstruction software and to explore
the potential of GPUs.  Increased code efficiency could result in
significant savings.  Also under consideration is saving 
more raw events to tape and only reconstructing them selectively.
The LHC produces about 15 PB of raw data per year now, but in 2021
the rate may rise to 130 PB.  Attention needs to be paid
to wide-area networking trends to assure that the network does not
become a bottleneck for the distributed centers used for event analysis.
It is also important to monitor cost and access rates of disk as
it seems that disk prices may not drop as rapidly as they have in the
past and access rates may also stagnate.  ATLAS currently spends
60\% more on disks than it does on CPUs.  In the future, it may be
best to recompute certain derived quantities rather than storing
results indefinitely.

The intensity frontier experiments do not match the computing requirements
of the energy frontier.  However, we found that
it would be desirable to better coordinate software development of
a number of intensity frontier experiments.  There is significant commonality
in different experiments' needs, and it would be useful to leverage
the experience and expertise of the energy frontier experiments
to increase the efficiency of the intensity frontier software development.

%%4) The price of storage and bandwidth of of disks may be of
%%significant concern in the future as prices may not drop as
%%rapidly as in the past and while disks have gotten bigger the
%%interface has not gotten significantly faster.

Cosmic frontier experiments will greatly expand their storage needs with the
development of new instruments.  Current data sets are about 1 PB, and the
total data set is expected to about 50 PB in ten years; however, in
10--20 years data will be collected at the rate of 400 PB/yr.
On the theoretical side, some of the most challenging simulations are
being run on supercomputers in the areas of astrophysics and cosmology.
Simulations to compare with observations and simulations to help design
new instruments will require increasing computing power and play a crucial
role in interpretation of experiments.  However,
as mentioned above, dealing with new computers architectures
will be challenging.  Growing archival storage, visualization of
simulations and allowing public access
to data are also issues that need attention.

Accelerator science is called on to both to simulate new accelerator designs
and to provide near-real-time simulations for operators of running accelerators to 
help them minimize beam loss.
It will be necessary to include additional physics in codes and
to improve algorithms to achieve these goals.  
Production runs can use from 10K to 100K core and about 140 M core-hrs 
will be used in 2013.  Considerable effort is being expended to 
port to new architectures.

Lattice field theory calculations rely on national supercomputer
centers and hardware purchased for the USQCD Computing Project.
Allocations at supercomputer centers exceed 500 M core-hrs in 2013.
The USQCD physics program provides essential input for interpretation
of a number of experiments and increased precision will be
required in the future.  For example, the $b$ quark mass and the
strong coupling $\alpha_s$ will need to be known at the
0.25\% level to compare coming LHC Higgs observations with the
predictions of the standard model.  Current precision is 0.5\%, and
planned work should yield the desired precision in the
appropriate time frame if computer power allocated increases as expected.
Advances in the calculation of hadronic contributions to muon $g-2$ will
be needed for interpretation of the experiment to be done at Fermilab.

Perturbative QCD is essential for theoretical understanding of collider
physics rates.
The perturbative QCD community took advantage of the Snowmass process to
port their codes to NERSC and OLCF, and to run on the Open Science Grid.  
Their needs are currently on the order 100K core-hrs to 1 M core-hrs
depending on the complexity of the final state and the order to
which the calculation is done.  They have also been benchmarking
GPU codes and finding impressive speed up over a single core.  A repository
of codes has been established at NERSC and a long term goal is to
make it easy for experimentalists to use these codes to compute
standard model rates for the processes they need.

The Distributed Computing and Facilities Infrastructures
subgroup looked at the growth trends in the Worldwide LHC
Computing Grid (WLCG) and the national high performance computing
(HPC) centers.  As mentioned above, most of the experimental computing
is of the HTP type, but it may be useful to do some of that work at
the HPC centers.  This could save smaller experiments from large
investments in hardware and personnel.  Grid computing has become
important in a number of areas of science, but HEP is still the
biggest user and must continue to be involved with grid computing
development.  The HPC computing done in theoretical physics will
require an order of magnitude increase in capacity and capability
at the HPC centers in the next five years and two orders of 
magnitude in the next ten years.

The Networking subgroup considered the distributed computing
model in use for large experiments and the implications for
network research and engagement with the National Research and
Education Networks (which carries most of our traffic).
Ten research questions were posed that should be answered before 2020.
There are ongoing reviews of networking needs done for ESnet.
It was also pointed out that expectations of network performance
need to be raised so that planning for network needs is on par with
that for computing and storage.  The gap between peak bandwidth and
delivered bandwidth should be narrowed.  It was not felt that wide-area
network performance will be an insurmountable bottleneck in the
next five to ten years as long as investments in higher performance
links continue.  However, there is uncertainty as to whether network
costs will drop at the same rate as they have done recently.

The Software Development, Personnel and Training subgroup has
made a number of recommendations to implement three main
goals.  First, use software development strategies and staffing models
that result in software more widely useful to the HEP
community.  Second, to be able to develop and support software
that will run with optimal efficiency on future computer architectures.
Third, to insure that developers and users have the training
necessary to deal with the increasingly complex software environments
and computing systems that will be used in the future.

The Storage and Data Management subgroup found that storage will
continue to be an expensive item for many experiments and that
rather than satisfy the real requirement for storage, it is
necessary to manage the cost to optimize the science output
from the experiment.  It is possible that tape, which continues
to be relatively inexpensive is underutilized.  While new disk
technology have been arriving every 18 months, there is now
less competition and the technology cycle may increase to three
years.  Distribute data management software can be costly for
an experiment, and research in the area could benefit a number
of experiments.

Computing will be essential for progress in theory and experiment
over the next two decades.  The advances in computer hardware that
we have seen in the past may not continue at the same rate in the
future.  The scientists who participated in the computing 
frontier have identified a number of issues as discusses above.
Early attention to these issues, has the potential to increase 
efficiency, reduce costs, enable significantly more realistic
theoretical calculations and avoiding computing bottlenecks in
the experimental program that could limit scientific progress.
