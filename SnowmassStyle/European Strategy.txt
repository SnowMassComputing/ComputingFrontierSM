2.9.4 Computing for Particle Physics 2020
Particle physics has relied on advances in computing to record and handle the large amounts of information generated by modern detectors and to model the physics processes and to simulate the interactions of particles in the detectors.   The field has taken advantage of and at times helped to develop new technologies.  The global computing infrastructure assembled for processing and analysis of LHC data has been hugely successful, and is an example of global planning and international partnerships for development, operations and funding of computing for science.

The LHC has been a great computing challenge for the particle physics community.  Experiments at LHC generate petabytes (PB) of data each year that are then processed, stored and made available to physicists around the world for analysis.   The data also will be archived for many years - at least for the lifetime of the experiments which could span several decades.  CERN and the rest of the HEP community responded to the computing challenge and developed the Worldwide LHC Computing Grid (WLCG).   This collaboration of institutes and national GRID consortia includes scientists and computing specialists from CERN, from computer centers across Europe and from around the world.  The WLCG came together to build and operate the infrastructure to manage the data produced by the LHC detectors and enabled the 1000s of scientists on the LHC to produce physics results and new discoveries at remarkable speed.    

The rapid evolution of computing technology is again expected to create many new challenges over the next decade.   At the same time the LHC experiments will continue to push the data rates to accommodate high intensity operations.   The field needs to update its computing strategy to be prepared to support the development and infrastructure needed to meet these upcoming challenges.

2.9.4.1 Computing Models

The model of HEP computing changed with the preparations for the LHC era.   Computing for particle physics is now distributed and takes advantage of excellent network bandwidth between sites and within each institute’s campus.    This global infrastructure was essentially a dream at the time the LHC experiments were initially planned. The GRID was an R&D project that held promise, but had not been tested at full scale. It took more than a decade to develop the plan and the corresponding core computing and software infrastructure to achieve the computing systems we nearly take for granted today. 

LHC computing routinely uses nearly 250,000 CPU processor cores, and nearly 170 petabytes of disk storage in addition to large multi petabyte tape libraries.   These resources are spread over many continents with prompt data processing and initial archiving of the data located at CERN.    The LHC model was based on the MONARC report [REF] that advocated a distributed system noting that power and cooling at CERN was limited.  It was also seen as beneficial to distribute the computing resources in order to take advantage of local expertise and funding.   The GRID was to provide the glue between the sites and good network connectivity was foreseen to be expensive with limited bandwidth between many sites.    The initial models were strictly hierarchical (Tier-0, Tier-1, Tier-2) with specific functionality assigned to each tier.  Data distribution had to be pre-planned and the computing tasks were sent to the data.   The computing models of today are based on an evolution of this paradigm that takes advantage of the excellent networking that is now available between most sites.

The operations of the LHC computing GRID worked well due to careful planning, adequate preparation of the sites and sufficient funding across the WLCG collaboration.  The funding for WLCG comes from a variety of sources including CERN, the EU, national funding agencies, regional GRID projects and local institutes.  The software and computing systems for physics analysis at the LHC were the result of a large worldwide effort that took years to plan and skilled scientists and engineers to commission it.  The coordinated effort paid off.  The LHC experiments had an accurate simulation based on GEANT4 at the time of first collisions and a tested data processing chain working at the right scale.   As a result, physicists from around the world could participate in analysis and the first physics papers were prepared within a few months. 

The LHC experiments plan for their computing systems to be in continuous operations throughout the next two decades.    There is a continuous need to improve the systems to make the most effective use of resources.   It is vital that the funding for the WLCG Tier-1 and Tier-2 centers be maintained at a level to ensure the full exploitation of the data produced by the LHC in the coming years.

Looking ahead to the computing challenges for the next decade, the HL-LHC stands out as a significant challenge.    The expected increases in trigger rate, pileup and detector complexity (number of channels) could increase the data rates by a about a factor of 10 or more.   This order of magnitude increase in storage and CPU requirements presents a new challenge for the computing infrastructure and the community will need time to prepare for it. The LHC community is beginning to review their computing models as they make plans for the next decade.  It is anticipated that the general design will be an evolution from the current models with the computing resources distributed at computing centers around the world. 

The general particle physics community should be able to profit from the e-infrastructure set up by the WLCG for the LHC computing.    The community needs a broader HEP-wide forum where strategic issues for computing for the next decade can be discussed and the common work coordinated.   It may be the right time to review the organizational structure of the WLCG in anticipation of the LHC future planning and development work that is getting underway.   

2.9.4.2 R&D for HEP Computing and Software

The development of the software for the LHC experiments spanned more than a decade and involved an international effort that partnered computing specialists with physicists to design, develop and commission the software systems.   The hardware components that were assembled into the computer centres were commercial products, but the software infrastructure to process, transport and manage the data was largely developed by the experiments or by the HEP GRID community and the related projects.   The funding for this development and deployment came from a variety of sources including the EU, CERN and the national funding agencies.  

For the past several decades, the community has been able to take advantage of developments in commercial computing components to expand dramatically the processing power, the storage capacity and network bandwidth capacity each year without a fundamental change in the software paradigm and without dramatic increases in funding.

Rapid improvements in CPU clock speeds and I/O speed are not expected to continue into the next decade.   We anticipate a need to reengineer many HEP software codes to adapt to the new computer architectures of the future since machines are expected to have many more cores per processor.   One of the consequences is that single event-by-event parallelism will no longer be efficient and we will need to adapt event processing software to implement sub-event parallelism and to take best advantage of the multi-core systems.  

The exploration of parallelism at the level of the individual algorithms demands software expertise beyond the skill of most physics graduate students and postdocs.  Software experts within the experiments are working to develop enhanced frameworks that mask the complications of parallelism from the users.   Simple schemes for module parallelism may not always be possible due to software module interdependence through algorithms such as for instance particle flow that combine tracking and calorimetry algorithms. For critical algorithms such as tracking, a partnership of physicists and experienced parallel programming experts will be needed to develop effective parallel codes.  The complete transformation of our physics software packages will take time, expertise and an adequate level of support.  There is already a HEP concurrency forum working in this direction. 

2..9.4.3 Infrastructure and Data Management

The LHC experiments and the WLCG have demonstrated that the particle physics community is able to manage large data samples effectively.   Particle physics has been a leader in this area within the science community for some time.  Many challenges lie ahead as data volumes continue to grow.   The community should continue to build on its successes and work to improve the efficiency through the development of optimized models for data placement, data caching and data access. 

The HEP computing and software community should take a leadership role in building collaborations with industry and with other data intensive sciences.   If the science community could identify more synergies among the communities and target specific areas where common tools for data management could be utilized, support and maintenance of the infrastructure could become more sustainable. The community should develop a plan to work with the wider scientific community to identify common data management solutions that could become open standards for scientific data management.

The WLCG infrastructure developed for the LHC should be open and available to the whole particle physics community.  This openness has already been partially achieved within many of the national grids.   Further technical development and planning is needed to make access to the global computing infrastructure more user-friendly for smaller collaborations.   One possible bi-product of HEP-wide sharing would be to make the e-infrastructure more open and attractive to other sciences. 

Cloud computing is under consideration as a possible component of HEP computing models of the future.   Although commercial clouds are not yet economically viable for large scale data intensive HEP computing, cloud computing, perhaps a “science cloud”, could well be a significant component of future HEP computing systems.  A pilot project to evaluate the use of cloud technologies for data processing has been launched as part of a European Cloud Computing partnership, Helix Nebula (www.helix-nebula.eu).

Network connectivity is expected to continue to improve and we can expect to have seamless remote access to data.   This will have an impact on the computing models for analysis and should lead to more efficient use of resources.  (The cost sharing of global network connectivity 

In general it takes 5 to 10 years to develop facilities and computing infrastructure.  The facilities of the next decade will have to be planned soon.  We need a strategic plan that captures HEP computing needs for the next decade.    CERN should work with the leaders of the national research infrastructure centres to develop the plan that best fits the needs of our science. 

2.9.4.4 Data Preservation and Data Access

Many HEP experiments have a lifecycle that is beyond the lifecycle of the computing technology we use.   Ideally data preservation should be built into each collaboration’s software plans from the beginning.  It is also becoming increasingly important to provide open access not only to science publications but also to the data.  It is not adequate to preserve an archived copy of the data; the associated software and software libraries also need to be preserved and well documented.   The HEP collaborations are aware of the need to develop clear policies and plans for data preservation and for open access.  This task has been taken up by the LHC experiments, where work is underway within the collaborations. 

The Study Group for Data Preservation and Long Term Analysis in High Energy Physics (DPHEP) has taken the lead in this important area.  HEP collaborations have traditionally tackled the preservation of data near the end of the experiment’s lifecycle.  This trend is changing since the need for data preservation and access is more widely recognized in large projects with long timescales.  CERN is working with DPHEP to provide some guidance for policies and leadership in the technical strategies for the current experiments. 

The HEP community has long advocated for open access to scientific publications in the field. LHC results are published with Open Access and there are plans to convert all HEP literature.  Open access to the data produced by particle physics experiments is becoming a requirement and will need to be built into models for data preservation.  Access to the raw data requires very specific knowledge of the characteristics of the detectors and accompanying detailed simulations and will not be as useful as processed and calibrated datasets.

INSPIRE (inspirehep.net) is the HEP digital library developed and operated in partnership among HEP institutions: CERN, DESY, Fermilab and SLAC.   INSPIRE is widely used in the community and is a model of successful infrastructure for scientific communication and knowledge management for other fields.  A sustainable funding model for these efforts is required for their continued success.

2.9.4.5 Particle Physics Software Libraries

Software libraries developed by particle physicists such as GEANT4, ROOT and event generators are widely used in the particle physics community and beyond.   They represent the work of years of many scientists and computing specialists.   These libraries are a repository of many of the tools and algorithms we need to do our work.   With adequate support, they will continue to evolve in order to keep up with the needs of the experiments.   They also need to run efficiently and to operate reliably on modern computer systems, which requires a model for sustained support.   It is essential that the community find a way to maintain and support these libraries, particularly the major toolkits and libraries that are in general use.   

GEANT4 has been a very successful detector simulation toolkit.   It was developed by a group of particle physicists working in a global collaboration.   The project has enjoyed the support of the experimental HEP community and many of the major laboratories in the field participate in the GEANT4 collaboration.   GEANT4 is widely used in the particle physics community and provides us with an unprecedented detailed understanding of detector systems.  In fact, detector simulations using GEANT4 code is responsible for a large part of the CPU cycles used by HEP. Therefore it is important that it runs very efficiently.   The core code and algorithms will need to be updated to take advantage of new computer architectures.   The codes will also have to accommodate descriptions of new devices and materials used in next generation detector R&D and new experiments.  Continued support is vital to this program.

CERN has been a leader in the development and preservation of the physics software libraries, mathematics and analysis toolkits libraries that have been developed for HEP specific applications and analysis.  There are now examples of software collaboration that involve more support from the HEP community.   It is important to foster this collaboration and to develop a sustainable model for support of our software over the long term. 

Summary

Computing for particle physics faces a number of new challenges including securing adequate funding for infrastructure, development and operations.    It is vital that the support for the operations teams and the WLCG centers be maintained at a level to ensure the full exploitation of the data produced by the LHC in the coming years.


