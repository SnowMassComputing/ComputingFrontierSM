
How do the different physics frontiers--and associated theory and physics simulation-
-differ in their needs for future computing technology evolution? In what respects
can they benefit from common computing technology evolution?}

We'll talk about that tomorrow in the computing summary, a lot of discussion

HTC: works well for EF, IF, 
	LHC success was a triumph for distributed HTC, even for "chaotic" analusys

HPC: leadership-class machine, allocation-request based computing for individual PIs


On the theory side, Large scale
calculations are going to be limited by the energy consumption of the
computer.  Writing efficient codes is likely to become more difficult as
we move to more exotic processors like GPUs or the Xeon Phi.  It is not
clear that one can abstract the details of the hardware in such a way
that a single code can be written for both those targets.  The Xeon Phi
requires three levels of parallelization for high performance.  SIMD
vectorization on the core,  OpenMP at the chip level and, message
passing such as MPI across chips.  It might be very important to develop
communication avoiding algorithms that use more flops but move fewer
bytes.  It is interesting to hear computer scientists project how many
picojoules are required to do a flop or move data from one node to
another.

Large scale simulations in the cosmic frontier and lattice gauge theory
are probably fairly similar.  

The comments above apply very well to Accelerator Science too.  We are already developing communication avoidance algorithms and doing the best we can to parameterize our data structures so we can have as much as possible "architecture independent" implementations, but there are limits as to how far these techniques can get you.  We have a fairly advanced R\&D program to port algorithms on the GPU and just starting on other options (Phi) and maybe, at the end, these are necessary pains until the dust settles, since to continue moving forward we do require large-scale computations. 

Cosmic frontier simulation codes face a similar situation as with lattice QCD and accelerators. We have some codes that are already fully exploiting GPUs and Xeon Phis and others that -- because of complexity issues -- are still waiting for the hardware and software mix to stabilize. In the realm of supercomputing, the technology evolution is out of our control, so whatever we get is what we will have to exploit. 

The IF experiments share the need of continued development of common HEP tools like Geant4 and ROOT with the other frontiers.  In addition, the IF experiments will heavily benefit from the development of a data handling system that is easily distributed and has transparent access for the user.  The example of the tiered computing used by the LHC experiments is a good basis for developing the model for the IF.  It is expected that any solution developed would provide access to the data as well as methods for submitting jobs to the grid. Advances in adapting key software tools to exploit multi-threading and GPU environments will also be beneficial across frontiers. 






While it may be true that leadership-class computers can run data-intensive
jobs, there are significant design differences. A single machine may be able to
handle traditional HPC tasks and data-intensive tasks, but the architectures
(and facilities) designed for number-crunching are very different from those
designed for data-intensive analysis. The conventional supercomputer is a
non-optimal device for data-intensive analysis (using various metrics:
flops/byte, I/O, storage), while the data server is not optimal for HPC
applications. And this is true even if both systems are based on the same "CPU".

Judging from tests by ATLAS of running Geant, ROOT, and ALPGEN on the ANL
"Intrepid" BlueGene/P, tightly-coupled leadership class systems such as those at
the OLCF and at the ALCF can provide effective high throughput computing for
experimental HEP. However, this is not seen as their mission, as they were
designed to give excellent performance on massively parallel codes (thousands to
hundreds of thousands of processors per job). Further, these facilities provide
important tightly-coupled computing resources for a number of areas of HEP,
including lattice QCD, cosmology, and accelerator simulation, and they are fully
subscribed via the INCITE program. Historically, the leadership class
supercomputer centers have not been particularly data-friendly, important for
high throughput computing, but this is changing rapidly with the realization
that an ability to treat "big data" problems is vital.

In technology, HEP clusters used for high throughput computing and leadership
class systems are converging. Often the CPUs are the same, and the problems of
poor utilization of CPU capabilities are common. Challenges such as "can we
exploit GPUs effectively?" are are also common ground, but the highly capable
intercommunications networks of leadership-class facilities remain largely
irrelevant to experimental HEP while being vital to theoretical calculations.

Bottom line, there will likely be a very large overlap between the work required
to make HEP code run effectively on an 2020 laptop and that required to exploit
a 2020 Leadership Class Facility.





-------------


While it may be true that leadership-class computers can run data-intensive jobs, there are significant design differences.
A single machine may be able to handle traditional HPC tasks and data-intensive tasks, but the architectures (and facilities) designed for number-crunching are very different from those designed for data-intensive analysis. The conventional supercomputer is a non-optimal device for data-intensive analysis (using various metrics: flops/byte, I/O, storage), while the data server is not optimal for HPC applications. And this is true even if both systems are based on the same "CPU". 
 On close examination, the leadership class systems, for example OLCF and
ALCF, can provide quite cost-effective throughput based computing for
experimental HEP, but this is not seen as their mission.  Historically,
the leadership class supercomputer centers have not been particularly
data-friendly, but this is changing rapidly with the realization that an
ability to treat "big data" problems is vital.

In technology, HEP clusters and leadership class systems are converging.
Often the CPUs are the same, and the problems of poor utilization of CPU
capabilities are common. Challenges such as "can we exploit GPUs
effectively?" are are also common ground, but the highly capable
intercommunications networks of leadership-class facilities remain largely
irrelevant to experimental HEP.

Bottom line, there is a very large overlap between the work required to
make HEP code run effectively on an 2020 laptop and that required to
exploit a 2020 Leadership Class Facility.

