%%%%% Magnetism Chapter  %%%%%%%%%%%%%%%%
 
\section{Computing for lattice field theory}
\label{chap:LFT}


One of the foremost goals of particle physics is to test the Standard Model
of particle physics and to search for indications of new physics beyond. In
many cases, interpretation of the experimental measurements requires a
quantitative understanding of the nonperturbative dynamics of the quarks and
gluons in the underlying process.  Lattice gauge theory provides the only
known method for \emph{ab initio} quantum chromodynamics (QCD) calculations
with controlled uncertainties, by casting the fundamental equations of QCD
into a form amenable to high-performance computing.  Thus, facilities for
numerical lattice gauge theory are an essential theoretical adjunct to the
experimental particle physics program.  Lattice QCD calculations now play
an essential role in the search for new physics at the Intensity Frontier.
They provide accurate results for many of the hadronic matrix elements needed
to realize the potential of present experiments probing the physics of
flavor. The methodology has been validated by comparison with a broad array of
measured quantities, several of which had not been well measured by experiment
at the time of the first precise lattice
calculations.  In the coming decades, lattice QCD will
play an expanded role in the search for new physics at both the Energy and
Intensity Frontiers.

The U.S. Lattice QCD Collaboration (USQCD), which consists of
most theoretical physicists in the country involved in the numerical studies
of QCD and beyond-the-Standard-Model theories, represents the lattice gauge
community.  Their efforts have been supported in an essential way by hardware
and software funding provided by the High Energy and Nuclear Physics Program
Offices of the Department of Energy.  The USQCD Collaboration's current
hardware project ends in FY2014, and the collaboration has applied for a
five-year project extension, ``LQCD-ext~II.''  The Collaboration's ongoing
software development and maintenance activities are supported by SciDAC-3
grants.

The report of the lattice field theory working group \cite{Blum:2013mhx}
summarizes the scientific
goals of the U.S. lattice gauge theory community, presents the current and
future computing needs and plans, and argues that continued support of the
U.S. (and worldwide) lattice-QCD effort is essential to fully capitalize on
the enormous investment in the particle physics experimental program.

\subsection{Lattice field theory scientific motivation}

Precision measurements at the Energy and Intensity Frontiers probe
quantum-mechanical loop effects, and are therefore sensitive to physics at
higher energy scales than those directly accessible at the LHC.  Contributions
from new heavy particles may be observable as deviations of the measurements
from Standard~Model expectations, provided both the experimental measurements
and theoretical predictions are sufficiently precise.  The scientific impact
of many future experimental measurements therefore hinges on reliable
Standard-Model predictions on the same time scale as the experiments and with
commensurate uncertainties.

For many quantities, the comparison between the measurements and
Standard-Model predictions are currently limited by theoretical uncertainties
from nonperturbative hadronic matrix elements or fundamental QCD parameters
that can only be computed numerically with lattice QCD.  The USQCD
Collaboration has laid out an ambitious vision for future lattice calculations
matched to the experimental priorities of the planned experimental high-energy
physics program over the next decade in the white papers ``Lattice QCD at the
Intensity Frontier'' and ``Lattice Gauge Theories at the Energy Frontier''
\cite{USQCD_IF_whitepaper13,USQCD_EF_whitepaper13}.  These detailed documents
present a concrete five-year plan for both the collaboration's foremost
scientific goals and the theoretical, algorithmic, and computational
strategies for achieving them.  The highest scientific priorities include
the following:

\begin{itemize}

\item Improving calculations of hadronic matrix elements involving
quark-flavor-changing transitions which are needed to interpret rare kaon
decay experiments

\item Improving calculations of the quark masses $m_c$ and $m_b$ and the
strong coupling $\alpha_s$ which contribute significant parametric
uncertainties to Higgs branching fractions

\item Calculating the nucleon axial form factor which is needed to improve
determinations of neutrino-nucleon cross sections for experiments such as
LBNE

\item Calculating the nucleon light- and strange-quark contents which are
needed to make model predictions for the $\mu \to e$ conversion rate at the
Mu2e experiment and to interpret dark-matter detection experiments in
which the dark-matter particle scatters off a nucleus

\item Calculating the hadronic light-by-light contribution to muon $g-2$, which
is needed to solidify and improve the Standard-Model prediction and interpret
the upcoming measurement as a search for new physics

\end{itemize}

Lattice field-theory calculations will also increasingly contribute to
collider experiments at the LHC 14-TeV run by providing quantitative
nonperturbative input for Higgs and other new-physics model building.

\subsection{Lattice field theory computing resources}

Substantial high-performance computing resources are needed to calculate
hadron masses and interactions with sufficient precision to test the Standard
Model against emerging experimental measurements.  Lattice gauge theory
simulations require parallel programming techniques, with the calculations
running cooperatively across hundreds to many thousands of processors or
processor cores.  The simulations must be run on hardware suitable for
massively parallel computations.  Although the simulations are
floating point intensive, on all current high-performance computing systems
throughput is limited by the rate that operands can be supplied to the
floating point execution units, either because of memory bandwidth limitations
or by the latency and bandwidth of interprocessor communications.
Interprocessor communications of data rely on message-passing
algorithms, typically implemented using an MPI~\cite{MPI} library.

At present, lattice theorists in the United States run these codes on a
variety of hardware.  The first type is commodity clusters based on Intel or AMD x86
processors and Infiniband networks, which have hundreds of nodes and
thousands of cores.  A second type is accelerated commodity clusters, similar to the the
standard clusters but with general purpose graphics processing units (GPUs) or
Intel Many Integrated Core (MIC) accelerators installed in each server; these
clusters have fewer nodes but typically hundreds of accelerators.  A third type is very-large-scale Cray supercomputers, consisting of thousands of AMD x86 processors
with a proprietary network, with the newest models also containing thousands
of GPUs.  Finally, lattice theorists use very large scale IBM BlueGene supercomputers, consisting of
hundreds of thousands of PowerPC cores interconnected on a proprietary
network.

Access to high-performance computing at both supercomputer ({\em capability})
and cluster ({\em capacity}) scales is essential for the lattice field theory
community.  A typical lattice-QCD analysis campaign involves a mix of problem
sizes.  The largest-scale computations are the generation of ensembles of
gauge fields, but at least as much integrated high-performance computing
capacity is required for the small- to large-scale parallel computations
(``analysis jobs'') to calculate different physical observables on these
ensembles.  In the U.S., the lattice community utilizes national
leadership-class supercomputing centers for the ensemble generation and for
the largest analysis jobs, as well as dedicated hardware purchased and
operated by USQCD for the much larger volume of small-to medium-scale analysis
jobs.

Table~\ref{tab:current} lists the leadership-class facility
capability and dedicated capacity resources utilized
for lattice-QCD simulations since 2010 by the USQCD
collaboration.  The capability resources are broken out showing both the ANL
and ORNL leadership class facilities; the capacity resources include all usage
on the DOE HEP- and NP-funded hardware at
Fermilab, Jefferson Lab, and BNL.  Subgroups within USQCD also use the DOE's
National Energy Research Scientific Computing Center (NERSC), centers
supported by the NSF's Extreme Science and Engineering Discovery Environment
(XSEDE) Program, and other facilities.

\begin{table}[t]
\begin{center}
\caption{Utilized core-hours of leadership-class facility
(LCF) and dedicated capacity hardware for lattice-QCD simulations.  The
conversion factors for lattice-QCD sustained Tflop/sec-years, assuming 8000
hours per year, is 1 Tflop/sec-year = 3.0M core-hour on BlueGene/Q hardware, and 1 Tflop/sec-year = 6.53M core-hour on
BlueGene/P and Cray hardware. Only USQCD-Collaboration resources are shown.
The drop in ANL LCF utilized capacity in 2012 occurred
because fewer opportunistic core-hours (``zero-priority queues'') were
available due to increased demand by other facility users.
\vspace{1.5mm}}
\label{tab:current}
\begin{tabular}{lccc}  
\hline\hline
Year & ANL LCF & ORNL LCF & Dedicated Capacity Hardware \\[-0.75mm]
& (BG/P + BG/Q core-hours) & (Cray core-hours) & (core-hours) \\[0.5mm]  \hline
2010 & 187M & 53.6M & 125M \\
2011 & 182M & 49.8M & 205M \\
2012 & 143M & 77.9M & 330M \\
2013 & 290M (allocated) & 140M (allocated) & 971M (planned) \\ \hline\hline
\end{tabular}
\end{center}
\end{table}

Because of the variety of processor types and parallel architectures,
efficient utilization of the above computing resources requires flexible and
effective software.  Since 2004, DOE grants to USQCD during the three
SciDAC~\cite{SciDAC} programs (2001--2006, 2006--2011, and 2011--2016) led to the
development of the USQCD software stack~\cite{SciDAC-software}.  This stack
includes low-level communications and I/O application program interfaces
(APIs) implemented via libraries ported to and optimized for each of the
architectures.  The stack includes linear algebra libraries with routines that
operate on single lattice sites, or across a full lattice with communications
between neighboring sites.  Lattice-QCD applications utilize the various
libraries of the software stack to run efficiently on any of the available
computing resources.  The USQCD software stack is a publicly available
resource supporting all of the main lattice gauge and fermion actions in
current use.  Further, it provides a general purpose framework that can be
extended to other quantum field theories besides QCD.

\begin{table}[t]
\begin{center}
\caption{Available resources for lattice-QCD simulations assumed for the
planned program of physics calculations.  The conversion factors for
lattice-QCD sustained Tflop/sec-years, assuming 8000 hours per year, is 1
Tflop/sec-year = 3.0M core-hour on BlueGene/Q hardware, and 1 Tflop/sec-year =
6.53M core-hour on BlueGene/P and Cray hardware. \vspace{1.5mm}}
\begin{tabular}{lccc}
\hline\hline  
Year & Leadership Class  & Dedicated Capacity Hardware  \\[-0.75mm] 
& (Tflop/sec-yrs) & (Tflop/sec-yrs) \\[0.5mm] \hline
2015 & 430 & 325 \\
2016 & 680 & 520 \\
2017 & 1080 & 800 \\
2018 & 1715 & 1275 \\ 
2019 & 2720 & 1900 \\ \hline\hline
\end{tabular}
\label{tab:fiveyear}
\end{center}
\end{table}
  
The planned U.S. scientific program in lattice field theory over the next
five years assumes the continued availability to USQCD of capability resources
at the DOE leadership class facilities, as well as the availability of
dedicated capacity resources at Fermilab, Jefferson Lab, and BNL, deployed and
operated under the proposed ``LQCD-ext~II'' project extension.  Table~\ref{tab:fiveyear}
shows the anticipated sustained LQCD Tflop/sec-yrs provided by these resources
by year.  Completion of the planned physics calculations will require well
over an order of magnitude of increased computing capacity beyond that used in
prior years.  Use of leadership-class facilities alone would provide
insufficient computational resources and would be unsuitable for the full mix
of lattice-field-theory job requirements.  Cluster-class parallel computing
hardware, including systems with GPU accelerators, delivers capacity with the
highest cost effectiveness for jobs ranging in size from tens to thousands of
cores.

Over an order of magnitude increase in storage utilization (disk and tape)
from the current approximately 2 petabyte usage will also be needed to support
the planned simulations.  Further, the anticipated evolution of high-performance computing hardware will require the evolution of software and the
introduction and refinement of new techniques and algorithms.  Positions for
postdocs and scientific staff to develop new lattice-gauge-theory code cannot
be supported by grants to lab and university theory groups alone, but must be
augmented through grant programs such as SciDAC.

\subsection{Lattice field theory summary}


Numerical lattice-QCD calculations are needed to interpret many upcoming
experimental measurements at the Energy and Intensity Frontiers as tests of
the Standard Model and new-physics searches.  Nonperturbative hadronic matrix
elements and fundamental QCD parameters enter the Standard~Model predictions
for many processes as diverse as rare kaon decays, Higgs branching fractions,
and the muon anomalous magnetic moment.  Thus, facilities for numerical
lattice gauge theory are an essential theoretical complement to the
experimental particle physics program.

The successful accomplishment of USQCD's scientific goals requires
access to both capacity and capability machines, and hence support for both
leadership-class facilities and dedicated computing clusters.  The combined
use of supercomputers to generate large suites of gauge fields and to perform
the largest analysis jobs with these ensembles, and dedicated lattice capacity
hardware to perform the much larger volume of small- to medium-scale analysis
jobs, is the most cost-effective model for lattice-field theory calculations.
The successful utilization of future computing resources requires software
that runs efficiently on new computing architectures, and hence support for
postdocs and scientific staff to develop lattice gauge theory code.

Support of USQCD through hardware and software grants, access to
leadership-class computing facilities, and funding of lab and university
theorists, is essential to fully capitalize on the enormous investments in the
DOE's high-energy physics and nuclear-physics experimental programs.

Given continued support of the lattice gauge theory effort in the U.S. and
worldwide, lattice calculations will play a key role in definitively
establishing the presence of physics beyond the Standard Model and in
determining its underlying structure.
