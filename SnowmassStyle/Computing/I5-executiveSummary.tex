\section{Storage and Data Management}


The largest HEP experiments have developed and are improving
functional distributed data and workflow management systems meeting
their needs. These systems are expensive to develop and operate and
are thus  rarely appropriate for smaller experiments.

HEP currently benefits from, but can also be  constrained by, the
highly successful ROOT features supporting reading  and writing of
persistent data. No other major scientific field uses  ROOT or appears
interested in it. Major developments in persistency  technology will
be required to take advantage of storage hardware on the  timescale of
LHC Run 3.

HEP should maintain and promote a vision of the future  in which fully
functional and low-operational-cost distributed computing and
persistency management is supported by software that is widely used in
data-intensive science.  To this end, developments in industry and the
wider  science community should be monitored actively, HEP should work
with the wider  science and computer science community to export and
adapt HEP technologies and  vice-versa. In distributed computing, HEP
should organize itself to significantly  reduce the number of diverse
approaches and provide the benefits of ideas and  software developed
in the largest experiments to other activities where they are needed.

Rotating disk storage will suffer a marked slowdown in the  evolution
of capacity/cost.  This may be the largest perturbation of HEP
computing  models that must attempt to optimize the roles of tape,
rotating disk, solid-state  storage, networking and CPU.

Many of the components required to support virtual data  already exist
in the data and workflow management software of the largest
experiments.   The rigorous provenance recording required to support
the virtual data concept would  also benefit data preservation.

Computing model implementations should be flexible  enough to adapt to
a wide range of relative costs of the key elements of HEP  computing.
In preparing for Run 3, the LHC program should seriously consider
virtual data as a way to accommodate scenarios where storage for
derived and  simulated data becomes relatively very costly.

All experiments across all frontiers
need  infrastructure that will allow scientists to store, catalog,
access, and  reprocess datasets years after the original physics
results are produced.  The inherent similarity of the requirements
across experiments and disciplines  call for a coordinated investment
in common infrastructure to enable easy  access and adoption of best
practices in knowledge preservation.  Solutions  should be developed
that meet the needs of the particle and astrophysics  communities
before widespread release of data to the public can be expected  or
mandated.

