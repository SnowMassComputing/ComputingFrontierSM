\section{Computing for Intensity Frontier Science}

Computing at the Intensity Frontier (IF) has many significant challenges. The experiments, projects and theory all require demanding computing capabilities and technologies.  Though not as data intensive as the LHC experiments, the IF experiments and IF computing have significant computing requirements in theory and modeling, beam line and experiment design, triggers and DAQ, online monitoring, event reconstruction and processing, and physics analysis.  It is critical for the success of the field that IF computing is modern, capable, has adequate capacity and support, and is able to take advantage of the latest developments in computing hardware and software advances.

This report will detail the computing requirements for the IF.  In \S~\ref{sec:overview}, a short overview of the the IF will be given.  An understanding of the IF program now and in the future is necessary to appropriately discuss the computing associated with IF.  The next section will discuss computing for the IF.  The emphasis here will be on the experiments, specifically on the aspects of computing required for IF experiments including the data handling architecture.  The following section summarizes information from IF experiments, current and planned, collected in a recent survey.  Next will come a discussion of some of the issues involved in computing for IF. This will include beam line design, simulation, demands for detector design, demands on Geant4, with particular emphasis on the importance of keeping up with current computing technology and techniques and aligning with the energy frontier computing whenever possible.  Finally there will be a summary.


\subsection{Overview of the Intensity Frontier: Recent Growth and Future Prospects}

The IF encompasses: $i)$ quark flavor physics, $ii)$ charged lepton processes, $iii)$ neutrinos, $iv)$ baryon number violation, $v)$ new light weakly coupled particles, and $vi)$ nucleons, nuclei and atoms~\cite{snowmass-if-document}. The requirements and resources of quark flavor physics, as in Belle II and LHCb, are more similar to those of the energy frontier. The requirements and resources of $iv)$ and $v)$ are more similar to those of the cosmic frontier. We have thus maintained focused on the areas of charged lepton processes, neutrinos, baryon number violation and nucleons, nuclei and atoms. 

The IF has become the central focus of the US-based particle physics program.  The transition to the IF dominated domestic program coincides with the transition at Fermilab from operating Energy Frontier (EF) experiments to operating IF experiments.  Many of the IF experiments are designed to measure rare processes by using very intense beams of particles.  Successful running of these experiments will involve not only the delivery of high intensity beams, but also the ability to efficiently store and analyze the data produced by the experiments. 

Several experiments comprise the Fermilab-based IF, including experiments to measure neutrino cross sections (MiniBooNE, MicroBooNE, MINER$\nu$A), experiments to measure neutrino oscillations over long (MINOS$+$, NO$\nu$A, LBNE) and short baselines (MiniBooNE, MicroBooNE), experiments to measure muon properties ($g-2$, $\mu2e$), other precision experiments (SEAQUEST), as well as future experiments (ORKA, $\nu$STORM).  Each of those experiments represent collaborations between 50 and 400 people.

There is also strong US participation in several international IF experiments, such as Super-Kamiokande (SK), T2K, Daya Bay, SNO/SNO+ as well as US university lead experiments such as IceCube. The impact of the US contribution to the physics results of these experiments is strongly correlated to the availability of computing resources and the efficiency of the computing model adopted. The groups participating in these experiments range in size from 30 to 250 people. In addition there is significant detector and experiment design R\&D. 

The IF has a large number of experiments and a range of scales in data output and throughput as well as number of experimenters. The situation is thus very different than at the EF which has just few experiments each of a very large scale. The number of experiments and range of scales can potentially lead to fragmentation, reinvention of the wheel, lack of access to computing advances and in general more dollars for each type of computing and personnel support needed. Furthermore, while there might be significant overlap of the human resources among experiments, there is little benefit when the tools and other resources used diverge significantly. A broad range of experiments leads to a wide breadth of needs, from support of tens to hundreds of experimenters, from high intensity realtime processing but small data sets to large data sets which in the sum are equivalent to the previous generation of collider experiments. Over the last few years there has been a significant effort by the IF experiments at Fermilab to join forces in using a more homogeneous set of software packages, frameworks and tools to access infrastructure resources. This trend has reduced fragmentation and led to more efficient use of resources. We would like to see this trend expand in the broader IF community adapted to the needs of each collaboration. 

