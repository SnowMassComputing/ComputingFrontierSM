\section{Distributed Computing and Facility Infrastructures}

Powerful distributed computing and robust facility infrastructures are essential to the continued progress of particle physics across all of its defined frontiers of experimental approaches.  Those approaches, and the theoretical work needed to support them, require a combination of both HTC and HPC systems.  The dominant consumers of HTC are the LHC experiments, who have been well served by it and should be in the future also.  Most intensity-frontier experiments can be supported by HTC also.  HPC is needed for applications such as lattice QCD, accelerator design and R\&D, data analysis and synthetic maps, N-body and hydro-cosmology simulations, supernova modeling, and more recently perturbative QCD.  HPC tasks have historically been carried out at national centers that have focused primarily on HPC facilities, but these centers have begun to embrace the problems that are addressed by HTC and are interested in attracting scientists who want to work at these centers.

Energy-frontier experiments face a growth in data that will make it a challenge to supply the needed computing resources.  Doing so is possible as long as certain conditions are in place and specific steps are taken to keep up with evolving technologies.  It requires near-constant funding of the WLCG, greater efficiencies in resource usage, and the evolution of software to take advantage of multicore processor architectures.  These experiments should also pursue and take advantage of opportunistic resources, be they in commercial clouds (which are not currently viable as purchased resources), universities, DOE centers or elsewhere.  The experiments would also benefit from further engagement with national HPC centers, which have resources available to HEP experiments and can support efforts to make use of HPC systems that have not traditionally been used for HTC applications for applications such as detector simulations.

Intensity-frontier experiments have smaller computing needs in comparison, and there is nothing technically that prevents them from being met.  Such experiments should be aware of the existence of resources available to them through the OSG or at national computing centers, and they could benefit from a coordinated effort amongst them to gain access to resources and share software and training.

Cosmic-frontier experiments advantage the simulations needed to interpret them are among the drivers of a need for growth in HPC resources in the coming years, along with lattice QCD and accelerator design.  Indeed, demand for access to HPC across HEP is expected to exceed the expected availability.  
Such computations are critically needed to interpret results from a number of important experiments and realize scientific returns from national investments in those experiments.  The NERSC report on HEP computing needs indicates a shortage of HPC resources needed for HEP by a factor of four by 2017.  While funding and technology development needed to sustain traditional HPC growth rates are uncertain, they must be maintained to support HEP science.

Distributed computing infrastructures, which have been critical to the success of the LHC experiments, should continue to be able to serve these and other applications even as they grow in scale.  There are no show-stoppers seen in increasing scale, but various developments should be purused to improve efficiency and ease of use.  Keeping sufficient staff support at a reasonable cost is a continuing concern; finding operational efficiencies could help address this.  Given that HEP is the largest user of distributed scientific computing, currently in the form of grid computing, members of the field must continue to take a leadership role in its development.

National centers play an important role in some aspects of computing, and HEP might be able to take advantage of an expanded role.  It is already used in many of the applications listed above.  While there are not enough resources dedicated to HEP available at the centers to rival those of the WLCG, experiments should explore the use of the centers as part of their efforts to diversify their computing architectures.  These centers do have access to large, state-of-the-art resources, operational support and expertise in many areas of computing.

We expect that distributed computing and facility infrastructures will continue to play a vital role in enabling discovery science.

