\section{Distributed Computing and Facility Infrastructures}

Powerful distributed computing and robust facility infrastructures are essential to the continued progress of particle physics across all of its defined frontiers of experimental approaches.  Those approaches, and the theoretical work needed to support them, require a combination of both High Throughput Computing (HTC) and and High Performance Computing (HPC) systems.  The dominant consumers of HTC are the LHC experiments, who have been well served by it and should be in the future also.  Most intensity-frontier experiments can be supported by HTC also.  HPC is needed for applications such as lattice QCD, accelerator design and R\&D, data analysis and synthetic maps, N-body and hydro-cosmology simulations, supernova modeling, and more recently, perturbative QCD.  HPC tasks have historically been carried out at national centers that have focused primarily on HPC facilities, but these centers have begun to embrace the problems that are addressed by HTC and are interested in attracting scientists who want to work at these centers.

Energy-frontier experiments face a growth in data that will make it a challenge to supply the needed computing resources.  Doing so is possible as long as certain conditions are in place and specific steps are taken to keep up with evolving technologies.  It requires near-constant funding of the Worldwide LHC Computing Grid (WLCG), greater efficiencies in resource usage, and the evolution of software to take advantage of multicore processor architectures.  These experiments should also pursue and take advantage of opportunistic resources, be they in commercial clouds (which are not currently viable and cost effective as purchased resources) or university and lab computing centers, or elsewhere.  The experiments would also benefit from further engagement with national HPC centers, which could make resource allocations available to HEP experiments and that have support start that could help efforts to port and integrate applications such as detector simulations that have not traditionally been used on HPC environments.

Intensity-frontier experiments have smaller computing needs in comparison, and there are no technical reasons why they could not be met.  Such experiments should become more pro-active to use resources that would be available to them through the Open Science Grid (OSG) or at national computing centers, and they could benefit from a coordinated effort amongst them to gain access to resources and share software and training.

Cosmic-frontier experiments and the simulations required to interpret them are among the drivers of a need for growth in HPC resources in the coming years, along with lattice QCD and accelerator design.  Demand for access to HPC across HEP frontiers is expected to exceed the amount of available resources expected.  HPC based computations are critically needed to interpret results from a number of important HEP experiments, to realize scientific returns from national investments in those experiments.  The NERSC report on HEP computing needs indicates a shortage of HPC resources needed for HEP by a factor of four by 2017.  While funding and technology development needed to sustain traditional HPC growth rates are uncertain, they must be maintained to support HEP science.

Distributed computing infrastructures, based at labs and universities, which have been critical to the success of the Energy Frontier experiments, should continue to be able to serve these and other applications even as they grow in scale.  There are no show-stoppers seen in increasing scale, but various developments should be pursued to improve efficiency and ease of use.  Keeping sufficient staff support at a reasonable cost is a continuing concern; finding operational efficiencies could help address this.  Given that HEP is the largest user of distributed scientific computing, currently in the form of HTC on computing grids, members of the field must continue to take a leadership role in its development.

National centers play an important role in some aspects of computing, and HEP might be able to take advantage of an expanded role.  Experiments should explore the use of the HPC centers as part of their efforts to diversify their computing architectures.  These centers do have access to large, state-of-the-art resources, operational support and expertise in many areas of computing.

We expect that distributed computing and facility infrastructures will continue to play a vital role in enabling discovery science.

