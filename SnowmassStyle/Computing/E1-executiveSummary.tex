%%%%% Computing Chapter  %%%%%%%%%%%%%%%%
 
\section{Computing for Cosmic Frontier Science}


The ``Cosmic Frontier'' (CF) is at the interface
between particle physics, cosmology, and astrophysics. 
Experimental and observational activities in the CF cover
laboratory experiments as well as multi-band observations of the
quiescent and transient sky. 
Direct dark matter search experiments,
laboratory tests of gravity theories, and accelerator dark matter
searches fall into the first class. Investigations of dark energy,
indirect dark matter detection, and studies of primordial fluctuations
fall into the second class; essentially the entire range of available
frequency bands is exploited, from the radio to TeV energies. Relevant
theoretical research also casts a very wide net---from quantum
gravity to the astrophysics of galaxy formation.

The size and complexity of CF experiments is also
diverse, ranging from the tabletop to large cosmological surveys,
simultaneously covering a number of precision measurements and
discovery-oriented searches. A defining characteristic of the CF 
is a trend towards ever larger and more complex observational
campaigns, with sky surveys now reaching collaboration memberships of
over a thousand researchers, of roughly the same size as a large high
energy physics experiment. Cross-correlating different survey
observations can extract more information, help to eliminate
degeneracies, and reduce systematic errors. These factors are among
the major drivers for the computational and data requirements that we
consider below.


The dramatic increase in data from CF experiments over
the last decade has led to fundamental breakthroughs in our knowledge
of the ``Dark Universe'' and physics at very high energies. Driven by
technological advances, current experiments generate in excess of a
petabyte of total data per year. Large format CCD cameras to measure
the growth of structure from weak gravitational lensing, wide-field
spectroscopic facilities to map the clustering of galaxies, increases
in the size of direct dark matter detectors, massive radio surveys,
and ground and space-based Cosmic Microwave Background 
experiments will continue this growth in data over the coming decade.

Regarding simulation facilities, 
the intrinsically observational nature of much of CF
science implies a great reliance on simulation and modeling. Not only
must simulations provide robust predictions for observations, they are
also essential in planning and optimizing surveys, and in estimating
errors, especially in the nonlinear domains of structure
formation. Synthetic sky catalogs play important roles in testing and
optimizing data management and analysis pipelines. The scale of the
required simulations varies from medium-scale campaigns for
determining covariance matrices to state of the art simulations for
simulating large-volume surveys, or, at the opposite extreme,
investigating dark matter annihilation signals from dwarf galaxies.

Required facilities for carrying out simulations include large-scale
supercomputing resources at DOE and NSF National Centers, local
clusters, and data-intensive computing resources needed to deal with
the enormous data streams generated by cosmological simulations. The
data throughput can easily exceed that of observations; data storage,
archiving, and analysis requirements (often in concert with
observational data) are just as demanding as for observational
datasets. Although there are significant challenges in fully
exploiting future supercomputing hardware, the expected resources
should satisfy simulation requirements, currently at the 10 PFlops
scale and expected to reach the exascale after 2020. The data-related
issues are more serious and will need changes in the current
large-scale computing model. Successful implementation of the recently
suggested Virtual Data Facility (VDF) capability at ALCF, NERSC, and
OLCF, would go a long way towards addressing these issues for CF 
simulations.

There is a continued growth in data from CF
experiments. 
Survey experiments currently exceed 1 PB of stored
data. Over the next decade the mass of data will exceed 100 PB. In
subsequent decades the development of radio experiments and energy
resolving detectors will result in an increase in data streaming to
$> 15$ GB/s.

Simulation requirements are projected to increase steeply. Current
allocations are estimated to be of the order of 200M compute
hours/year, with associated storage in the few PB range, and a shared
data volume of the order of 100TB. Data management standards and
software infrastructure vary widely across research teams. The
projected requirements for 2020 are an order of magnitude improvement
in data rates (to 10--100 GB/s), a similar increase in peak
supercomputer performance ($\approx 200$ PFlops), and the ability to store and
analyze data sets in the 100 PB class. It is difficult to make precise
estimates for 2030 as hardware projections are hazy, however, the
science requirements based on having complete datasets from missions
such as LSST, Euclid, and large radio surveys would argue for at least
another order of magnitude increase across the board.


The use of computational resources will need to grow to match the
associated data rates for the processing and analysis of observational
data and for simulated astrophysical and cosmological processes. Most
(but not all) of the data processing pipelines use linear time
algorithms, thus the amount of processing is roughly proportional to
the amount of data collected by the instruments. Exceptions to the
linear law will be algorithms which will incrementally reprocess all
the data from a given instrument over and over, whose processing
capabilities must therefore grow as a quadratic function of time.

Most pipelines can be characterized by the number of cycles needed to
process a byte of data. Typical numbers in astrophysics today range
from a few thousand to 100K cycles, thus to process a canonical 100 PB
data set, 10$^{22}$ cycles, or about a billion CPU hours, are
required. One particular characteristic of this processing is that it
will require a reasonable, but not excessive sequential I/O rate to
the disks the data is stored on, typically less than a GB/s per
processing node.

Much of this processing is massively parallel, and thus will execute
very well on SIMD architectures. Emerging many-core platforms will
likely have a huge impact on the efficiency of pipeline
computing. While these platforms are harder to code against, pipeline
codes will be based on well-architected core libraries, where it will
be cost efficient to spend resources to optimize their parallel
execution, thus substantially decreasing the hardware investment.


The projected
data volumes for archiving of observational data 
are not particularly large compared to
commercial data sets (with the possible exception of SKA). Given that
the eventual data volumes will probably exceed a few exabytes, the
analyses must be co-located with the data.
The most likely high-level architecture for scientific analyses will
be a hierarchy of tiers, in some ways analogous to the LHC computing
model, where the Tier 0 data is a complete capture of all raw data,
but then derived and value added data products are moved and analyzed
further at lower tiers of the hierarchy, which are not necessarily
co-located with the Tier 0 data centers.

The archives will have to be based upon intelligent services, where
heavy indexing can be used to locate and filter subsets of the
data. There is a huge growth in the diversity of such ``Big Data
Analytics'' frameworks, ranging from petascale databases (SciDB,
Dremel, etc.) to an array of NoSQL solutions. Over the next 5 years a
few clear winners will emerge, allowing the research community to
leverage the best solutions. A high speed, reliable and
inexpensive networking infrastructure connecting the instruments and
all the sites involved in the archiving will be crucial to the success
of the entire enterprise.

Today's architectures for data analysis and simulations include
supercomputers, suitable for massive parallel computations where the
number of cycles per byte of data is huge, possessing a large
distributed memory, but with a relatively small amount of on-line
storage. Database servers occupy the opposite range of the spectrum,
with a very large amount of fast storage, but not much processing
power on top of the data. For most scientific analyses the required
architecture lies somewhere in between these two: it must have a large
sequential I/O speed to petabytes of data, and also perform very
intense parallel computations. 

Fast graph processing will become increasingly important as both large
and complex simulations are analyzed and as one tracks complex
spatio-temporal connections among objects detected in multi-band
time-domain surveys. To efficiently execute algorithms that require
large matrices and graphs, it is likely that a combination of large
(multiple TB) memory (RAM) will be combined with multiprocessors to
minimize communication overhead. Also, new storage technologies with
fast random access (SSD, memory bus flash, phase change memory, NVRAM)
will play a crucial role in the storage hierarchy.

Large-scale data sets, arising from both simulations and experiments,
present different analysis tasks requiring a variety of data access
patterns. These can be subdivided into three broad categories.

Some of the individual data accesses will be very small and localized,
such as accessing the properties of individual halos, or galaxies, and
recomputing their observational properties. These accesses typically
return data in small blocks, require a fast random access, a high IOPS
rate and are greatly aided by good indexing. At the same time there
will be substantial computation needed on top of the small data
objects. These accesses can therefore benefit from a good underlying
database system with enhanced computational capabilities. Going beyond
the hardware requirements, this is an area where the clever use of
data structures will have an enormous impact on the system
performance, and related algorithmic techniques will be explored
extensively. The challenge here is that the small data accesses will
be executed billions of times, suggesting a parallel, shared database
cluster with a random access capability of tens of millions of IOPS
and a sequential data speed of several hundred GB/s, with an unusually
high computing capability inside the servers themselves.

At the other end of the spectrum are the analyses that will have to
touch a large fraction, possibly all of the data, like computing an
FFT of a scalar field over the entire volume, or computing correlation
functions of various orders, over different subclasses of
objects. These require very fast streaming access to data, algorithms
that can compute the necessary statistics over (possibly multiple)
streams, and hardware that can handle these highly parallelizable
stream computations efficiently (multiprocessors). Here the
requirements would be a streaming data rate in excess of 500 GB/s
between the data store and the processing, and a peak processing
capability of several PFlops over vectorizable code. These patterns
map best onto traditional HPC systems, with the caveat of the extreme
data streaming requirements.

The third type of access pattern is related to rendering computer
graphics. These tasks will generate various maps and projections,
touching a lot of data, and typically generating 2D images. Such tasks
include computing maps of dark matter annihilation in large
simulations with trillions of particles, ray-tracing to compute
gravitational lensing over a large simulation, ray-traced simulated
images for future telescopes, based on simulations and detailed
telescope and atmospheric models. As many of these tasks are closely
related to computer graphics, mapping to GPU hardware will be very
important, as this approach can yield performance gains of well over
an order of magnitude.

Dealing with each of these access patterns demands substantial
investments in hardware and software development. To build an
efficient streaming engine, every one of the bottlenecks, both in
hardware and software, must be eliminated as a single chokepoint can
seriously degrade the performance of the whole system. In terms of
algorithms, many traditional RAM-resident algorithms must be recast
into streaming versions. A rethink of statistical algorithm design is
needed, and computations (and computability) should be explicitly
included into the cost tradeoffs.

The need for better programming models, and better high-level
abstractions is evident. In a complex, massively parallel system it
will become increasingly difficult to write code explicitly
instructing the hardware. Therefore, there is a need to explore and
embrace new declarative programming models where the explicit
execution of the code is transparent to the use. At a higher level,
there is a pressing need for the development of a sustainable software
effort that can provide a baseline of support to multiple experiments,
with experiment-specific extensions being built on top of such a
capability. This will require a community effort in the development
and exploitation of new algorithms, programming models, workflow
tools, standards for verification, validation, and code testing, and
long-term support for maintenance and further development of the
resulting software base.

Simulation plays a critical role in
CF science, not only as the primary tool for theoretical
predictions, but even more significantly, in evaluating and
interpreting the capabilities of current and planned experiments. For
optical surveys, the chain begins with a large cosmological simulation
into which galaxies and quasars (along with their individual
properties) are placed using semi-analytic or halo-based models. A
synthetic sky is then created by adding realistic object images and
colors and by including the local solar and galactic
environment. Propagation of this sky through the atmosphere, the
telescope optics, detector electronics, and the data management and
analysis systems constitutes an end-to-end simulation of the survey. A
sufficiently detailed simulation of this type can serve a large number
of purposes such as identifying possible sources of systematic errors
and investigating strategies for correcting them and for optimizing
survey design (in area, depth, and cadence). The effects of systematic
errors on the analysis of the data can also be investigated; given the
very low level of statistical errors in current and next-generation
precision cosmology experiments, and the precision with which
deviations from $\Lambda$CDM are to be measured, this is an absolutely
essential task.

Directly analogous to the situation in building a community-supported
software base for CF experiments, there is a related need
for bringing together larger collaborations in the area of
simulations. The LQCD community has shown what is possible in this
direction by working together in a large national collaboration. Such
efforts are now beginning within the CF and will
hopefully come to fruition in the near term.


While much of the science in CF is undertaken by small
groups of physicists, the collaborations themselves have grown to
hundreds and sometimes thousands of members. Many of the techniques
utilized by these collaborations are common to multiple CF 
experiments. Most experiments have, however, developed their
analysis and processing software independently of other CF 
programs. This can lead to duplication of effort, software
that is tailored only to meet a specific need, non-scalable
approaches, and software that is difficult to sustain beyond the
lifetime of an individual experiment. Programs for sustainable
software should, therefore, be considered by HEP in order to make
computing developments more robust. To ensure that the software and
techniques maintained by these programs are successful will require a
community that actively uses and develops within these tools (as has
been demonstrated, for example, by the GEANT4 development). 


\begin{center}
\begin{table}
\begin{tabular}{|l|r|r|r|} 
 \hline 
{\bf Experimental Data} & 2013 & 2020 & 2030+ \\
\hline
Storage & 1 PB & 6 PB & 100--1500 PB \\
Cores & 10$^3$ & 70K & 300+K \\
CPU hours & 3x10$^6$ hrs & $2\times 10^8$ hrs & $\sim 10^9$ hrs \\
{\bf Simulations} &&& \\
Storage & 1--10 PB & 10--100 PB & $> 100 $PB -- 1EB\\
Cores & 0.1--1M & 10--100M &$> 1$G\\
CPU hours & 200M & $>$20G & $> 100$G\\
\hline
\end{tabular}
\label{tab:CompNeeds}
\caption{Compute needs for Cosmic Frontier Science in 10--20 years.}
\end{table}
\end{center}

