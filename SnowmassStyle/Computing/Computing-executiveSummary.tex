

\section{Executive Summary}

Computing has become a major component of all particle physics experiments
and in many areas of theoretical particle physics. The Computing Frontier
study group established subgroups covering user needs and infrastructure.
The user needs groups covered the experiments at the energy and intensity
frontiers, the combined needs for cosmic frontier experiments, astrophysics
and cosmology, and theory subgroups covered accelerator science,
astrophysics and cosmology, lattice field theory, and perturbative QCD.
Four infrastructure groups examined trends in computing to predict how
technology will evolve and how it will affect future costs and
capabilities. These groups focused on distributed computing and facility
infrastructures; networking; software development, personnel, and training;
data management and storage. These groups looked to identify critical
technology needs for particle physics that might require the DOE or NSF to
fund research in computer science and technology.

During the period between the Community Planning Meeting at Fermilab and
the Community Summer Study at the University of Minnesota, the Computing
Frontier groups were actively engaged with the other Frontiers to learn of
their plans and estimate their computing needs. The infrastructure groups
engaged with vendors, computer users, providers and technical experts to
predict trends in computing, networking, storage, and software development,
including considerations of costs, capacities and speeds. At the Minnesota
meeting two days of parallel sessions were devoted to discussions across
the the subgroups, to finalize subgroup findings, and to identify common
trends and needs. A Computing Frontier summary talk was prepared and
presented at the plenary session.

Progress in HEP experiment and theory will require significantly more
computing, software development, storage and networking, with different
projects stretching future capabilities in different ways. However, there
are many commonalities in the needs between the different areas in HEP, and
in the future more commonality and community planning is advised to move
ahead in the most efficient manner. This requires careful and continuing
review of the topics we studied, {\it i.e.}, user needs and capabilities of
current and future technology. For many years, the particle physics
community has been a great source of computing innovation and expertise. It
is essential to leverage those assets through wider sharing of knowledge
throughout the experimental and theoretical communities. 
We should be open to bi-directional sharing of expertise with the entire
%community of data-intensive science."
scientific community.

The experimental program relies for the most part on distributed
high-throughput computing (HTC). Data analysis, simulation and
reconstruction of individual events are independent of each other, and
groups of events can be assigned to hardware in different locations.
Results are combined together when all event groups are done. This
distributed computing model was pioneered by the energy frontier
experiments and is relying on a distributed infrastructure of computing
centers as part of Open Science Grid in the U.S. and extending across the
globe. The theoretical computing and simulation needs are more commonly
addressed by high-performance computing (HPC) in which thousands to hundred
thousands of tightly coupled CPUs are working simultaneously on a single
problem. These resources are provided mostly through DOE and NSF
supercomputing centers.

One issue for those applications that traditionally rely on HTC for their
data-intensive computing is to what degree they can or should use national
supercomputer centers which have traditionally been designed for HPC usage.
Work is proceeding to make these HTC applications run on HPC, and to
interface HPC centers to the HTC workload and data management
infrastructures. The question of cost effectiveness is being discussed.
Also, traditional HPC applications are developing more data-intensive
science needs, which, however, are currently not a good match to existing
and next-generation HPC architectures. Computational resources will have to
address the demands for greatly increasing data rates, and the increased
needs for data intensive computing tasks like data analytics, for comparing
large samples of simulations and observational data, etc.

Another pressing issue facing both HTC and HPC communities is that
processor speeds are no longer increasing exponentially, as they were for
at least two decades. Instead, new chip architectures provide multiple
cores. Thus, we cannot rely on new hardware to run serial codes faster, and
we must parallelize codes to increase application performance. In addition
to multi-core chips, there are accelerators such as graphical processing
units (GPUs) and many-core chips such as the Intel Xeon Phi. 
In the past computing resource needs for energy frontier experiments scaled
roughly with the rate that processor speeds increased, following Moore's
law. In the future this requires full use of multiple-core and many-thread 
architectures. Also
in the storage area scaling of disk capacity and throughput is of
significant concern, as per-unit capacities no longer increase as rapidly.

These changes in chip technology and high performance system architectures
require us to develop parallel algorithms and codes, and to train personnel
to develop, support and maintain them. Different subgroups are at different
stages in their efforts to port to these new technologies. Lattice QCD, for
example, started its GPU porting efforts in 2008 and has had code in
production for some time, particularly for quark inversions; however, there
are other parts of the code that are still only running on CPUs.
Cosmological simulations have exploited GPUs since 2009 and some 
codes have fully incorporated GPUs in their production versions, running 
at full scale on hybrid supercomputers.
Accelerator science is also actively porting codes to GPUs. Some of the
solvers and particle-in-cell infrastructures have been ported and very
significant speed-ups have been obtained. The perturbative QCD community
has also started using GPUs.

These trends lead to vastly increasing code and system complexities. For
example, only a limited number of people in the field can program GPUs. In
this and other highly technical areas, developing and keeping expertise in
new software technologies is a challenge, while well-trained personnel and
key developers are leaving to take attractive positions in industry.
Continued training is an important aspect, and there are training materials
from some of the national supercomputing centers and summer schools are
organized by, among others, the Virtual School of Computational Science and
Engineering (www.vscse.org). We must examine whether these provide the
right training for our field and whether the delivery mechanisms are
timely. Using on-line media, workbooks and wikis were suggested to enhance
training. Another area of common concern is the career path of those who
become experts in software development and computing. It is useful to help
young scientists learn computing and software skills that are marketable
for non-academic jobs, but it is also important that there be career paths
within particle physics, including tenure-track jobs, for those working at
the forefront of computation.

For the {\it Energy Frontier} experiments computing limitations already
cause a limit to the amount of physics data that can be analyzed. The
planned upgrades to the LHC energy and luminosity are expected to result in
a ten-fold increase in the number of events and a ten-fold increase in
event complexity. Effort has begun to increase code efficiency and
parallelism in reconstruction software and to explore the potential of
GPUs. It is being considered to save more raw events to tape and only
reconstruct them selectively. The LHC produces about 15 PB of raw data per
year now, but in 2021 the rate may rise to 130 PB. Attention needs to be
paid to data management and wide-area networking, to assure that network
connectivity does not become a bottleneck for the distributed event
analysis. It is important to monitor storage cost and throughputs. More
than half of the computing cost is now for storage, and in the future it
may become cost-effective to recompute certain derived quantities rather
than storing them.

{\it Intensity Frontier} experiments have combined computing requirements
on the scale of a single energy frontier experiment but they are a more
diverse set than the energy frontier. A survey was conducted and found that
there is significant commonality in different experiments' needs. Sharing of
resources across experiments, like in the Open Science Grid, is a first step
in addressing peak compute resources needs.  Continued coordination of
software development between these experiments will allow for efficiently
developed coding infrastructure.  Leveraging the data handling experience and
expertise of the energy frontier experiments for the diverse intensity
frontier experiments would significantly improve their ability to reconstruct
and analyze data.

{\it Cosmic Frontier} experiments will greatly expand their storage needs
with the start of new surveys and the development of new instruments.
Current data sets are about 1 PB, and the total data set is expected to be
about 50 PB in ten years. Beyond that, in 10--20 years data will be
collected at the rate of 400 PB/yr. On the astrophysics and cosmology
theory side, some of the most challenging simulations are being run on
supercomputers. 
Current allocations for this effort are approximately 200M core-hours annually.
Very large simulations will require increasing computing
power. Comparing simulations with observations will play a crucial role in
interpretation of experiments, and simulations are needed to help design
new instruments. There are very significant challenges in dealing with new
computers architectures and very large data sets, as described above.
Growing archival storage, visualization of simulations and allowing public
access to data are also issues that need attention.

{\it Accelerator Science} is called on to simulate new accelerator designs
and to provide near-real-time simulations feedback for accelerator
operation. 
Research into new algorithms and designs has potential to bring new ideas and
capabilities to the field.
It will be necessary to include additional physics in codes and
to improve algorithms to achieve these goals. Production runs can use from
10K to 100K cores. Considerable effort is being expended to port to new
architectures, in particular to address the real-time requirements.

{\it Lattice Field Theory} calculations rely on national supercomputer
centers and hardware purchased for the USQCD Computing Project. Allocations
at supercomputer centers have exceeded 500 M core-hrs this year, and
resource requests will go up by a factor of 50 by the end of this decade.
This program provides essential input for interpretation of a number of
experiments, and increased precision will be required in the future. For
example, the $b$ quark mass and the strong coupling $\alpha_s$ will need to
be known at the 0.25\% level, a factor of two better than now, to compare
upcoming ILC Higgs observations with Standard Model predictions. Advances
in the calculation of hadronic contributions to muon $g-2$ will be needed
for interpretation of the planned experimental measurement.

{\it Perturbative QCD} is essential for theoretical understanding of
collider physics rates. Codes were ported to the HPC centers at NERSC and
OLCF, and also run on the Open Science Grid. They have also been
benchmarking GPU codes and finding impressive speed up over a single core.
A repository of codes has been established at NERSC and a long term goal is
to make it easy for experimentalists to use these codes to compute Standard
Model rates for the processes they need.

The {\it Distributed Computing and Facilities Infrastructures} subgroup
looked at the growth trends in distributed resources as provided by the
Open Science Grid, and the national high performance computing (HPC)
centers. Most of the computing by experiments is of the HTC type, but HPC
centers could be used for specific work flows. Using existing computing
centers could save smaller experiments from large investments in hardware
and personnel. Distributed HTC has become important in a number of science
areas outside HEP, but HEP is still the biggest user and must continue to
drive the future computing development. HPC computing needs for theoretical
physics will require an order of magnitude increase in capacity and
capability at the HPC centers in the next five years, and two orders of
magnitude in the next ten years.

The {\it Networking} subgroup considered the implications of distributed
computing on network needs, required R\&D and engagement with the National
Research and Education Networks (which carries most of our traffic). A
number of research questions were formulated that need to be answered
before 2020. Expectations of network performance should be raised so that
planning for network needs is on par with that for computing and storage.
The gap between peak bandwidth and delivered bandwidth should be narrowed.
It was not felt that wide-area network performance will be an
insurmountable bottleneck in the next five to ten years as long as
investments in higher performance links continue. However, there is
uncertainty as to whether network costs will drop at the same rate as they
have done in the past.

The {\it Software Development, Personnel and Training} subgroup has a
number of recommendations to implement three main goals. The first goal is
to use software development strategies and staffing models that result in
software more widely useful to the HEP community. The second goal is to
develop and support software that will run with optimal efficiency on
future computer architectures. The third goal is to insure that developers
and users have the training necessary to deal with the increasingly complex
software environments and computing systems that will be used in the future.

The {\it Storage and Data Management} subgroup found that storage continues
to be a cost driver for many experiments. It is necessary to manage the
cost to optimize the science output from the experiment. Tape storage
continues to be relatively inexpensive and should be more utilized within
the storage hierarchy. 
Disk storage is likely to increase in capacity/cost relatively slowly due
to a shrinking consumer market and technology barriers.
Operating distributed data management systems can be costly for
experiments, and continued R\&D in this area would benefit a number of 
experiments.

To summarize, the challenging resource needs for the planned and proposed
physics programs require efficient and flexible use of all resources. HEP
needs both distributed HTC and HPC. Emerging experimental programs might
consider a mix to fulfill demands. Programs to fund these resources need to
continue. Sharing and opportunistic use help address resource needs, from
all tiers of computing, eventually including commercial providers. There is
increasing need for data intensive computing in traditionally
computation-intensive fields, including at HPC centers, for data analytics,
combining simulations and observational data, etc.

In order to satisfy our increasing computational demands, the field needs
to make better use of advanced computing architectures. With the need for
more parallelization, the complexity of software and systems continues to
increase, impacting architectures for application frameworks, workload
management systems, and also the physics code. We must develop and maintain
expertise across the field, and re-engineer frameworks, libraries and
physics codes. Unless corrective action is taken to enable us to take full
advantage of the new hardware architectures, we could be frozen out of cost
effective computing solutions on a time scale of 10 years. There is a large
code base that needs to be re-engineered, and we currently do not have
enough people trained to do it.

The continuing huge growth in observational and simulation data drives the
need for continued R\&D investment in data management, data access methods,
and networking. Continued evolution of the data management and storage
systems will be needed in order to take advantage of new network
capabilities, ensure efficiency and robustness of the global data
federations, and to contain the level of effort needed for operations.
Significant challenges with data management and access remain, and research
into these areas could continue to bring benefit across the Frontiers.  We
expect solutions that will be based on content delivery approaches, dynamic
data placement, and remote data access.

Network reliability is essential for data intensive distributed computing.
Emerging network capabilities and data access technologies improve our
ability to use resources independent of location. This will enable use of
diverse compute resources: dedicated facilities, university computing
centers, opportunistic use of shared resources between PIs, even scientific
communities, commercial clouds, eventually also making leadership-class HPC
centers relevant for data-intensive computing. The computing models should
treat networks as a resource that needs to be managed and planned for.

Computing will be essential for progress in theory and experiment over the
next two decades. The field continues to learn how to do more science with
constrained resources, requiring us to be more flexible and perhaps tolerate
higher levels of risk. The advances in computer hardware that we have seen
in the past may not continue at the same rate in the future. The issues
identified in this report require continuing attention. Addressing them
will increase efficiency, reduce costs, and enable us to meet the
experimental and theoretical goals identified through the Snowmass process.

