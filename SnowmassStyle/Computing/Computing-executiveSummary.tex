

\section{Introduction}

Computing has become a major component of all particle physics experiments
and many areas of theoretical particle physics. 
Progress in particle physics experiment and theory will require significantly more
computing, software development, storage, and networking, with different
projects stretching future capabilities in different ways.  
As a result of considerable work throughout the Snowmass process, we
recommend improved training, more community planning, careful and continuing 
review of the topics outlined in more detail below, and expanded efforts 
to share our expertise with and learn from experts beyond our field.
%%%However, there
%%%are many common needs among different areas in particle physics, so more
%%%community planning is advised to increase efficiency.
%%%Careful and continuing review of the topics we studied, 
%%%{\it i.e.}, user needs and capabilities of
%%%current and future technology, is needed.  For many years, the particle physics
%%%community has been a great source of computing innovation and expertise. 
%%%We must continue to share our expertise within our field and beyond.
%%%We should find more opportunties to learn from others as well.

The Computing 
subgroups covered user needs and infrastructure.
Experimental user needs subgroups included those for
the Cosmic, Energy and Intensity Frontiers.
Theory subgroups covered accelerator science,
astrophysics and cosmology, lattice field theory, and perturbative QCD.
Four infrastructure groups predicted computing trends and
how they will affect future costs and
capabilities. These groups focused on distributed computing and facility
infrastructures; networking; software development, personnel, and training; and
data management and storage. 
The Computing
subgroups engaged with the other frontiers to learn of
their plans and to estimate their computing needs.  The infrastructure groups
engaged with vendors, computer users, providers, and technical experts. 

Our study group
considered the hardware and software needs of particle physics for the next
ten to twenty years, and recommends, not a grand project or two, but a continuing
process of monitoring and supporting the hardware and software needs of the field
in order to optimize the contribution of computing to scientific output.
One difference between computing and other enabling technologies is that there is
a vibrant global computer industry that is constantly creating new products
and improving current ones.  Although some of the computing equipment that we deploy
is customized, such as application-specific integrated circuits, the vast
majority of it is widely available.  We are not building a bespoke detector with
a multi-decade lifetime.  For the most part, we are purchasing commercially available
computing equipment that will last less than ten years.  However, we need to
carefully examine our computing needs and the available technology
to ensure that our systems are configured to cost-effectively meet those needs.
In contrast to the short lifetime of the hardware, the
software that is developed for both experiment and theory has a longer lifetime. 
However, the software is undergoing continual development and optimization.   

Two different styles of computing are currently used in particle physics.
The experimental program mainly relies on distributed
high-throughput computing (HTC). 
The distributed computing model was pioneered by Energy Frontier
experiments.  It relies on distributed computing
centers that are part of the Open Science Grid in the U.S.,
with additional centers across the globe. 
The theoretical computing and simulation needs are more commonly
addressed by high-performance computing (HPC) in which many
tightly coupled central processing units (CPUs) are working together on a single
problem. These resources are provided mostly through DOE and NSF
supercomputing centers.

One important issue to consider is 
to what degree can or should data-intensive applications that have traditionally 
relied on HTC use national supercomputer centers, which have traditionally 
been designed for HPC usage?
Work is proceeding to determine how well and how cost-effectively these HTC
applications can run at HPC centers.
Also, traditional HPC applications are developing more data-intensive
science needs, which are currently not a good match to existing
and next-generation HPC architectures. Computational resources will have to
address the demands for greatly increasing data rates, and the increased
needs for data-intensive computing tasks.

Another pressing issue facing both HTC and HPC communities is that
processor speeds are no longer increasing, as they were for
at least two decades. Instead, new chips provide multiple
cores. Thus, we cannot rely on new hardware to run serial codes faster.
We must therefore parallelize codes to increase application performance. 
Today's computer servers contain one or more multi-core chips.  Currently,
these chips  have up to 10 (Intel) or 16 (AMD) cores. 
For additional performance the server may contain
computational accelerators such as graphical processing
units (GPUs) or many-core chips such as the Intel Xeon Phi that can have up
to 61 cores. 
In the past, computing resource needs for Energy Frontier experiments scaled
roughly with the rate that processor speeds increased, following Moore's
law. In the future, this requires full use of multiple-core and many-thread 
architectures. Also,
scaling of disk capacity and throughput is of
significant concern, as per-unit capacities will no longer increase as rapidly
as they have in the past.

These changes in chip technology and high-performance system architectures
require us to develop parallel algorithms and codes, and to train personnel
to develop, support and maintain them. Different subgroups are at different
stages in their efforts to port to these new technologies. 
In the U.S., the effort to write lattice QCD codes, for
example, started in 2008 and there has been code in
production for some time; however, there
are other parts of the code that are still only running on CPUs.
Cosmological simulations have exploited GPUs since 2009, and some 
codes have fully incorporated GPUs in their production versions, running 
at full scale on hybrid supercomputers.
Accelerator science is also actively writing codes for GPUs. Some of the
solvers and particle-in-cell infrastructures have been ported and very
significant speed-ups have been obtained. The perturbative QCD community
has also started using GPUs.

These trends lead to vastly increasing code and system complexities. For
example, only a limited number of people in the field can program GPUs. In
this and other highly technical areas, developing and keeping expertise in
new software technologies is a challenge, because well-trained personnel and
key developers are leaving to take attractive positions in industry.
Continued training is important.  There are training materials
from some of the national supercomputing centers.  Summer schools are
organized by the Virtual School of Computational Science and
Engineering (www.vscse.org) and other groups. 
We must examine whether these provide the
right training for our field and whether the delivery mechanisms are
timely.  On-line media, workbooks and wikis were suggested to enhance
training. Another area of common concern is the career path of those who
become experts in software development and computing. We should help
young scientists learn computing and software skills that are marketable
for non-academic jobs, but it is also important that there be career paths
within particle physics, including tenure-track jobs, for those working at
the forefront of computation.

Subsequent sections of this chapter summarize the finding of each of our subgroups.
We start with the needs of the Energy, Intensity, and Cosmic Frontiers.  We then
turn to the needs of theoretical areas: accelerator science, lattice field theory,
and perturbative QCD.  Theoretical work in astrophysics and cosmology is included
in the Cosmic Frontier section.  
Our last section briefly summarizes our conclusions.
Additional details appear in individual sections and in the full subgroup
reports.
